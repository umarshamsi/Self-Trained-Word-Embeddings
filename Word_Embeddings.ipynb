{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.9 64-bit ('PythonCPU': conda)",
   "display_name": "Python 3.7.9 64-bit ('PythonCPU': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c6a09f3073d02787a25c7a7bf1e02884604b283b7eb1c4f65f86d04c9cd6c325"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # For preprocessing\n",
    "import pandas as pd  # For data handling\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "import spacy  # For preprocessing\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(12999, 20)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df = pd.read_csv(r'D:\\Word Embeddings\\fake.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "uuid                  0\n",
       "ord_in_thread         0\n",
       "author                0\n",
       "published             0\n",
       "title                 0\n",
       "text                  0\n",
       "language              0\n",
       "crawled               0\n",
       "site_url              0\n",
       "country               0\n",
       "domain_rank           0\n",
       "thread_title          0\n",
       "spam_score            0\n",
       "main_img_url          0\n",
       "replies_count         0\n",
       "participants_count    0\n",
       "likes                 0\n",
       "comments              0\n",
       "shares                0\n",
       "type                  0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()\n",
    "df.isnull().sum()\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time to clean up everything: 22.2 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4597, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 16:27:54: 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "l',\n",
       "  'pillar',\n",
       "  'long',\n",
       "  'political',\n",
       "  'career',\n",
       "  'fact',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'politician',\n",
       "  'america',\n",
       "  'associated',\n",
       "  'abortion',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'roe',\n",
       "  'v',\n",
       "  'wade',\n",
       "  'decide',\n",
       "  'million',\n",
       "  'baby',\n",
       "  'murder',\n",
       "  'united',\n",
       "  'state',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  's',\n",
       "  'hand',\n",
       "  'drench',\n",
       "  'blood',\n",
       "  'vote',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'hand',\n",
       "  'drench',\n",
       "  'blood',\n",
       "  'needless',\n",
       "  'absolutely',\n",
       "  'horrified',\n",
       "  'prominent',\n",
       "  'evangelical',\n",
       "  'leader',\n",
       "  'come',\n",
       "  'support',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'election',\n",
       "  'season',\n",
       "  'example',\n",
       "  'group',\n",
       "  'represent',\n",
       "  'latino',\n",
       "  'evangelical',\n",
       "  'church',\n",
       "  'announce',\n",
       "  'endorse',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'organization',\n",
       "  'represent',\n",
       "  'latino',\n",
       "  'evangelical',\n",
       "  'church',\n",
       "  'u',\n",
       "  's',\n",
       "  'endorse',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'statement',\n",
       "  'thursday',\n",
       "  'group',\n",
       "  'open',\n",
       "  'usa',\n",
       "  'say',\n",
       "  'clinton',\n",
       "  'prove',\n",
       "  'willingness',\n",
       "  'engage',\n",
       "  'difficult',\n",
       "  'conversation',\n",
       "  'listen',\n",
       "  'contrast',\n",
       "  'opinion',\n",
       "  'engage',\n",
       "  'faith',\n",
       "  'leader',\n",
       "  'evangelical',\n",
       "  'leader',\n",
       "  'recently',\n",
       "  'sign',\n",
       "  'petition',\n",
       "  'change',\n",
       "  'org',\n",
       "  'strongly',\n",
       "  'denounce',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'undersigned',\n",
       "  'evangelical',\n",
       "  'simply',\n",
       "  'tolerate',\n",
       "  'racial',\n",
       "  'religious',\n",
       "  'gender',\n",
       "  'bigotry',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'consistently',\n",
       "  'deliberately',\n",
       "  'fuel',\n",
       "  'matter',\n",
       "  'choose',\n",
       "  'vote',\n",
       "  'vote',\n",
       "  'truly',\n",
       "  'alarming',\n",
       "  'trend',\n",
       "  'see',\n",
       "  'election',\n",
       "  'season',\n",
       "  'number',\n",
       "  'prominent',\n",
       "  'woman',\n",
       "  'evangelical',\n",
       "  'movement',\n",
       "  'openly',\n",
       "  'reject',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'embrace',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'follow',\n",
       "  'short',\n",
       "  'excerpt',\n",
       "  'recent',\n",
       "  'washington',\n",
       "  'post',\n",
       "  'article',\n",
       "  'examine',\n",
       "  'phenomenon',\n",
       "  'jen',\n",
       "  'hatmaker',\n",
       "  'speak',\n",
       "  'stadium',\n",
       "  'christian',\n",
       "  'woman',\n",
       "  'regale',\n",
       "  'story',\n",
       "  'child',\n",
       "  'garden',\n",
       "  'austin',\n",
       "  'tex',\n",
       "  'stay',\n",
       "  'away',\n",
       "  'politic',\n",
       "  'recently',\n",
       "  'take',\n",
       "  'facebook',\n",
       "  'instagram',\n",
       "  'blast',\n",
       "  'donald',\n",
       "  'j',\n",
       "  'trump',\n",
       "  'national',\n",
       "  'disgrace',\n",
       "  'remind',\n",
       "  'legion',\n",
       "  'follower',\n",
       "  'name',\n",
       "  'ballot',\n",
       "  'november',\n",
       "  'christianity',\n",
       "  'today',\n",
       "  'recently',\n",
       "  'publish',\n",
       "  'editorial',\n",
       "  'female',\n",
       "  'evangelical',\n",
       "  'leader',\n",
       "  'entire',\n",
       "  'country',\n",
       "  'publicly',\n",
       "  'endorse',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'accord',\n",
       "  'christianity',\n",
       "  'today',\n",
       "  'deborah',\n",
       "  'fike',\n",
       "  'permanent',\n",
       "  'representative',\n",
       "  'united',\n",
       "  'nation',\n",
       "  'world',\n",
       "  'evangelical',\n",
       "  'alliance',\n",
       "  'represent',\n",
       "  'constituency',\n",
       "  'million',\n",
       "  'alliance',\n",
       "  'office',\n",
       "  'country',\n",
       "  'fik',\n",
       "  'say',\n",
       "  'step',\n",
       "  'leadership',\n",
       "  'position',\n",
       "  'openly',\n",
       "  'advocate',\n",
       "  'clinton',\n",
       "  'recent',\n",
       "  'resignation',\n",
       "  'evangelical',\n",
       "  'leadership',\n",
       "  'position',\n",
       "  'endorse',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'speak',\n",
       "  'volume',\n",
       "  'important',\n",
       "  'believe',\n",
       "  'elect',\n",
       "  'november',\n",
       "  'toxic',\n",
       "  'tone',\n",
       "  'atmosphere',\n",
       "  'surround',\n",
       "  'mr',\n",
       "  'trump',\n",
       "  'fuel',\n",
       "  'supporter',\n",
       "  'irreparable',\n",
       "  'damage',\n",
       "  'country',\n",
       "  'future',\n",
       "  'gop',\n",
       "  'public',\n",
       "  'witness',\n",
       "  'evangelical',\n",
       "  'america',\n",
       "  'see',\n",
       "  'big',\n",
       "  'supporter',\n",
       "  'question',\n",
       "  'mind',\n",
       "  'spirit',\n",
       "  'overwhelming',\n",
       "  'challenge',\n",
       "  'american',\n",
       "  'president',\n",
       "  'face',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'qualified',\n",
       "  'person',\n",
       "  'run',\n",
       "  'oval',\n",
       "  'office',\n",
       "  'issue',\n",
       "  'national',\n",
       "  'security',\n",
       "  'economic',\n",
       "  'stability',\n",
       "  'see',\n",
       "  'healthcare',\n",
       "  'reform',\n",
       "  'continue',\n",
       "  'forward',\n",
       "  'tackle',\n",
       "  'domestic',\n",
       "  'challenge',\n",
       "  'poverty',\n",
       "  'inequality',\n",
       "  'racism',\n",
       "  'need',\n",
       "  'person',\n",
       "  'occupy',\n",
       "  'office',\n",
       "  'lot',\n",
       "  'woman',\n",
       "  'think',\n",
       "  'abortion',\n",
       "  'shouldn',\n",
       "  't',\n",
       "  'major',\n",
       "  'issue',\n",
       "  'election',\n",
       "  'like',\n",
       "  'say',\n",
       "  'holocaust',\n",
       "  'shouldn',\n",
       "  't',\n",
       "  'major',\n",
       "  'issue',\n",
       "  'nazi',\n",
       "  'germany',\n",
       "  'look',\n",
       "  'don',\n",
       "  't',\n",
       "  'vote',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'good',\n",
       "  'christian',\n",
       "  'cast',\n",
       "  'vote',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'cast',\n",
       "  'vote',\n",
       "  'evil',\n",
       "  'wicked',\n",
       "  'corrupt',\n",
       "  'politician',\n",
       "  'nation',\n",
       "  'possibly',\n",
       "  'see',\n",
       "  'publicly',\n",
       "  'endorse',\n",
       "  'sinful',\n",
       "  'position',\n",
       "  'proud',\n",
       "  'stand',\n",
       "  'know',\n",
       "  'write',\n",
       "  'election',\n",
       "  'lot',\n",
       "  'lately',\n",
       "  'feel',\n",
       "  'important',\n",
       "  'medium',\n",
       "  'coverage',\n",
       "  'focus',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'feel',\n",
       "  'election',\n",
       "  'far',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'thing',\n",
       "  'husband',\n",
       "  'document',\n",
       "  'american',\n",
       "  'people',\n",
       "  'willingly',\n",
       "  'choose',\n",
       "  'know',\n",
       "  'exactly',\n",
       "  'unfortunately',\n",
       "  'christian',\n",
       "  'stand',\n",
       "  'unite',\n",
       "  'clinton',\n",
       "  'political',\n",
       "  'divide',\n",
       "  'evangelical',\n",
       "  'christian',\n",
       "  'world',\n",
       "  'grow',\n",
       "  'deep',\n",
       "  'reach',\n",
       "  'liberty',\n",
       "  'university',\n",
       "  'following',\n",
       "  'come',\n",
       "  'atlantic',\n",
       "  's',\n",
       "  'big',\n",
       "  'deal',\n",
       "  'week',\n",
       "  'ago',\n",
       "  'group',\n",
       "  'liberty',\n",
       "  'student',\n",
       "  'letter',\n",
       "  'explain',\n",
       "  'stand',\n",
       "  'republican',\n",
       "  'presidential',\n",
       "  'nominee',\n",
       "  'jerry',\n",
       "  'falwell',\n",
       "  'jr',\n",
       "  'run',\n",
       "  'school',\n",
       "  'father',\n",
       "  'die',\n",
       "  'announce',\n",
       "  'support',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'january',\n",
       "  'speak',\n",
       "  'candidate',\n",
       "  's',\n",
       "  'behalf',\n",
       "  'interview',\n",
       "  'event',\n",
       "  'liberty',\n",
       "  'student',\n",
       "  'disappointed',\n",
       "  'president',\n",
       "  'falwell',\n",
       "  's',\n",
       "  'endorsement',\n",
       "  'tired',\n",
       "  'associate',\n",
       "  'bad',\n",
       "  'presidential',\n",
       "  'candidate',\n",
       "  'american',\n",
       "  'history',\n",
       "  'student',\n",
       "  'write',\n",
       "  'donald',\n",
       "  'trump',\n",
       "  'represent',\n",
       "  'value',\n",
       "  'want',\n",
       "  'thousand',\n",
       "  'people',\n",
       "  'sign',\n",
       "  'letter',\n",
       "  'include',\n",
       "  'student',\n",
       "  'say',\n",
       "  'roughly',\n",
       "  'student',\n",
       "  'alumnus',\n",
       "  'liberty',\n",
       "  'edu',\n",
       "  'email',\n",
       "  'address',\n",
       "  'dustin',\n",
       "  'wahl',\n",
       "  'alex',\n",
       "  'forbe',\n",
       "  'letter',\n",
       "  's',\n",
       "  'author',\n",
       "  'feature',\n",
       "  'msnbc',\n",
       "  'cnn',\n",
       "  'say',\n",
       "  'receive',\n",
       "  'supportive',\n",
       "  'email',\n",
       "  'tweet',\n",
       "  'russell',\n",
       "  'moore',\n",
       "  'head',\n",
       "  'political',\n",
       "  'arm',\n",
       "  'southern',\n",
       "  'baptist',\n",
       "  'convention',\n",
       "  'erick',\n",
       "  'erickson',\n",
       "  'conservative',\n",
       "  'radio',\n",
       "  'host',\n",
       "  'support',\n",
       "  'clinton',\n",
       "  'particularly',\n",
       "  'strong',\n",
       "  'young',\n",
       "  'adult',\n",
       "  'evangelical',\n",
       "  'read',\n",
       "  'follow',\n",
       "  'paragraph',\n",
       "  'website',\n",
       "  'new',\n",
       "  'york',\n",
       "  'time',\n",
       "  'absolutely',\n",
       "  'astounded',\n",
       "  'kate',\n",
       "  'shellnutt',\n",
       "  'online',\n",
       "  'editor',\n",
       "  'christianity',\n",
       "  'today',\n",
       "  'editor',\n",
       "  'ct',\n",
       "  'woman',\n",
       "  'section',\n",
       "  'say',\n",
       "  'observe',\n",
       "  'millennial',\n",
       "  'generation',\n",
       "  'lot',\n",
       "  'patience',\n",
       "  'trump',\n",
       "  'influential',\n",
       "  'millennial',\n",
       "  'evangelical',\n",
       "  'profile',\n",
       "  'cover',\n",
       "  'story',\n",
       "  'year',\n",
       "  'ago',\n",
       "  'say',\n",
       "  'find',\n",
       "  'lila',\n",
       "  'rise',\n",
       "  'pro',\n",
       "  'trump',\n",
       "  'publicly',\n",
       "  'critical',\n",
       "  'hashtag',\n",
       "  'nevertrump',\n",
       "  'ms',\n",
       "  'shellnutt',\n",
       "  'say',\n",
       "  'frightening',\n",
       "  'thing',\n",
       "  'election',\n",
       "  'chance',\n",
       "  'evangelical',\n",
       "  'christian',\n",
       "  'shape',\n",
       "  'political',\n",
       "  'direction',\n",
       "  'nation',\n",
       "  'truth',\n",
       "  'demographic',\n",
       "  'rapidly',\n",
       "  'shift',\n",
       "  'include',\n",
       "  'demographic',\n",
       "  'evangelical',\n",
       "  'community',\n",
       "  'robert',\n",
       "  'jone',\n",
       "  'expertly',\n",
       "  'document',\n",
       "  'recent',\n",
       "  'book',\n",
       "  'end',\n",
       "  'white',\n",
       "  'christian',\n",
       "  'america',\n",
       "  'number',\n",
       "  'old',\n",
       "  'conservative',\n",
       "  'white',\n",
       "  'male',\n",
       "  'evangelical',\n",
       "  'shrink',\n",
       "  'year',\n",
       "  'number',\n",
       "  'young',\n",
       "  'evangelical',\n",
       "  'ethnic',\n",
       "  'background',\n",
       "  'moral',\n",
       "  'political',\n",
       "  'view',\n",
       "  'extend',\n",
       "  'far',\n",
       "  'position',\n",
       "  'gay',\n",
       "  'marriage',\n",
       "  'abortion',\n",
       "  'rise',\n",
       "  'follow',\n",
       "  'work',\n",
       "  'regularly',\n",
       "  'know',\n",
       "  'little',\n",
       "  'hope',\n",
       "  'future',\n",
       "  'america',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'elect',\n",
       "  'exactly',\n",
       "  'zero',\n",
       "  'hope',\n",
       "  'evangelical',\n",
       "  'christian',\n",
       "  'stand',\n",
       "  'united',\n",
       "  'stop',\n",
       "  'point',\n",
       "  'appear',\n",
       "  'go',\n",
       "  'happen',\n",
       "  'courtesy',\n",
       "  'michael',\n",
       "  'snyder',\n",
       "  'not',\n",
       "  'forget',\n",
       "  'follow',\n",
       "  'd',\n",
       "  'c',\n",
       "  'clothesline',\n",
       "  'facebook',\n",
       "  'twitter',\n",
       "  'help',\n",
       "  'spread',\n",
       "  'word',\n",
       "  'share',\n",
       "  'article',\n",
       "  'favorite',\n",
       "  'social',\n",
       "  'network',\n",
       "  'share'],\n",
       " ['world',\n",
       "  'change',\n",
       "  'drastically',\n",
       "  'ready',\n",
       "  'future',\n",
       "  'doesn',\n",
       "  't',\n",
       "  'need',\n",
       "  've',\n",
       "  'tell',\n",
       "  'rise',\n",
       "  'technology',\n",
       "  'real',\n",
       "  'time',\n",
       "  'pressure',\n",
       "  'online',\n",
       "  'global',\n",
       "  'economy',\n",
       "  'human',\n",
       "  'clever',\n",
       "  'careful',\n",
       "  'leave',\n",
       "  'future',\n",
       "  'perspective',\n",
       "  'charge',\n",
       "  'human',\n",
       "  'labor',\n",
       "  'lose',\n",
       "  'value',\n",
       "  'people',\n",
       "  'liability',\n",
       "  'documentary',\n",
       "  'reveal',\n",
       "  'real',\n",
       "  'motivation',\n",
       "  'secretive',\n",
       "  'effort',\n",
       "  'reduce',\n",
       "  'population',\n",
       "  'bring',\n",
       "  'resource',\n",
       "  'use',\n",
       "  'strict',\n",
       "  'centralized',\n",
       "  'control',\n",
       "  'big',\n",
       "  'threat',\n",
       "  'face',\n",
       "  'isn',\n",
       "  't',\n",
       "  'automation',\n",
       "  'robot',\n",
       "  'destroy',\n",
       "  'job',\n",
       "  'large',\n",
       "  'sense',\n",
       "  'human',\n",
       "  'obsolete',\n",
       "  'altogether'],\n",
       " ['post',\n",
       "  'october',\n",
       "  'daisy',\n",
       "  'luther',\n",
       "  'let',\n",
       "  's',\n",
       "  'talk',\n",
       "  'wikileak',\n",
       "  'organization',\n",
       "  'found',\n",
       "  'julian',\n",
       "  'assange',\n",
       "  'website',\n",
       "  'explain',\n",
       "  'wikileak',\n",
       "  'specializ',\n",
       "  'analysis',\n",
       "  'publication',\n",
       "  'large',\n",
       "  'dataset',\n",
       "  'censor',\n",
       "  'restrict',\n",
       "  'official',\n",
       "  'material',\n",
       "  'involve',\n",
       "  'war',\n",
       "  'spying',\n",
       "  'corruption',\n",
       "  'far',\n",
       "  'publish',\n",
       "  'million',\n",
       "  'document',\n",
       "  'associate',\n",
       "  'analysis',\n",
       "  'year',\n",
       "  've',\n",
       "  'publish',\n",
       "  'document',\n",
       "  'disproven',\n",
       "  'single',\n",
       "  'time',\n",
       "  'record',\n",
       "  'authentication',\n",
       "  'perfect',\n",
       "  'learn',\n",
       "  'mean',\n",
       "  'person',\n",
       "  'pretty',\n",
       "  'silly',\n",
       "  'disregard',\n",
       "  'ream',\n",
       "  'information',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'democratic',\n",
       "  'party',\n",
       "  'clinton',\n",
       "  'foundation',\n",
       "  'political',\n",
       "  'shenanigan',\n",
       "  'machiavelli',\n",
       "  'shame',\n",
       "  'important',\n",
       "  'thing',\n",
       "  'come',\n",
       "  'hillary',\n",
       "  'clinton',\n",
       "  'unfortunately',\n",
       "  'report',\n",
       "  'mainstream',\n",
       "  'interest',\n",
       "  'brevity',\n",
       "  'topic',\n",
       "  'link',\n",
       "  'article',\n",
       "  'go',\n",
       "  'deeper',\n",
       "  'leak',\n",
       "  'particular',\n",
       "  'order',\n",
       "  'john',\n",
       "  'podesta',\n",
       "  'chairman',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'nice',\n",
       "  'cozy',\n",
       "  'dinner',\n",
       "  'peter',\n",
       "  'kadzik',\n",
       "  'official',\n",
       "  'department',\n",
       "  'justice',\n",
       "  'day',\n",
       "  'benghazi',\n",
       "  'hearing',\n",
       "  'kadzik',\n",
       "  's',\n",
       "  'son',\n",
       "  'ask',\n",
       "  'job',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'icing',\n",
       "  'corruption',\n",
       "  'cupcake',\n",
       "  'kadzik',\n",
       "  'lead',\n",
       "  'effort',\n",
       "  'nominate',\n",
       "  'loretta',\n",
       "  'lynch',\n",
       "  'famously',\n",
       "  'meet',\n",
       "  'bill',\n",
       "  'clinton',\n",
       "  'private',\n",
       "  'plane',\n",
       "  'right',\n",
       "  'hillary',\n",
       "  's',\n",
       "  'interrogation',\n",
       "  'emailgate',\n",
       "  'source',\n",
       "  'know',\n",
       "  'clinton',\n",
       "  'foundation',\n",
       "  'way',\n",
       "  'clinton',\n",
       "  'family',\n",
       "  'launder',\n",
       "  'money',\n",
       "  's',\n",
       "  'proof',\n",
       "  'zero',\n",
       "  'hedge',\n",
       "  'write',\n",
       "  'today',\n",
       "  's',\n",
       "  'wikileak',\n",
       "  'dump',\n",
       "  'include',\n",
       "  'memo',\n",
       "  'reveal',\n",
       "  'time',\n",
       "  'precise',\n",
       "  'financial',\n",
       "  'flow',\n",
       "  'clinton',\n",
       "  'foundation',\n",
       "  'band',\n",
       "  's',\n",
       "  'firm',\n",
       "  'teneo',\n",
       "  'consulting',\n",
       "  'clinton',\n",
       "  'family',\n",
       "  's',\n",
       "  'private',\n",
       "  'business',\n",
       "  'endeavor',\n",
       "  'pundit',\n",
       "  'call',\n",
       "  'leak',\n",
       "  'rosetta',\n",
       "  'stone',\n",
       "  'clinton',\n",
       "  'foundation',\n",
       "  'meaning',\n",
       "  'document',\n",
       "  'shady',\n",
       "  'financial',\n",
       "  'dealing',\n",
       "  'unravel',\n",
       "  'translate',\n",
       "  'source',\n",
       "  'clinton',\n",
       "  'unable',\n",
       "  'speak',\n",
       "  'long',\n",
       "  'podium',\n",
       "  'lean',\n",
       "  'numerous',\n",
       "  'leak',\n",
       "  'email',\n",
       "  'reference',\n",
       "  'certain',\n",
       "  'interview',\n",
       "  'keep',\n",
       "  'short',\n",
       "  'd',\n",
       "  'article',\n",
       "  'reference',\n",
       "  'interesting',\n",
       "  'reason',\n",
       "  'case',\n",
       "  'surprisingly',\n",
       "  'isn',\n",
       "  't',\n",
       "  'relate',\n",
       "  'health',\n",
       "  'source',\n",
       "  'leak',\n",
       "  'clinton',\n",
       "  'intend',\n",
       "  'good',\n",
       "  'restrict',\n",
       "  'second',\n",
       "  'amendment',\n",
       "  'brian',\n",
       "  'fallon',\n",
       "  'national',\n",
       "  'press',\n",
       "  'secretary',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'write',\n",
       "  'circle',\n",
       "  'gun',\n",
       "  'follow',\n",
       "  'friday',\n",
       "  'morning',\n",
       "  'discussion',\n",
       "  'today',\n",
       "  'indicate',\n",
       "  'definitely',\n",
       "  'plan',\n",
       "  'ask',\n",
       "  'bout',\n",
       "  'gun',\n",
       "  'discussion',\n",
       "  'news',\n",
       "  'event',\n",
       "  'previous',\n",
       "  'time',\n",
       "  'discuss',\n",
       "  'gun',\n",
       "  'go',\n",
       "  'background',\n",
       "  'reporter',\n",
       "  'tonight',\n",
       "  'specific',\n",
       "  'proposal',\n",
       "  'support',\n",
       "  'president',\n",
       "  'universal',\n",
       "  'background',\n",
       "  'check',\n",
       "  'course',\n",
       "  'close',\n",
       "  'gun',\n",
       "  'loophole',\n",
       "  'executive',\n",
       "  'order',\n",
       "  'impose',\n",
       "  'manufacturer',\n",
       "  'liability',\n",
       "  'accord',\n",
       "  'analysis',\n",
       "  'daily',\n",
       "  'sheeple',\n",
       "  'impose',\n",
       "  'manufacturer',\n",
       "  'liability',\n",
       "  'mean',\n",
       "  'sandy',\n",
       "  'hook',\n",
       "  'bushmaster',\n",
       "  'remington',\n",
       "  'arm',\n",
       "  'prosecute',\n",
       "  'have',\n",
       "  'hand',\n",
       "  'murder',\n",
       "  'child',\n",
       "  'school',\n",
       "  'staff',\n",
       "  'member',\n",
       "  'firearm',\n",
       "  'legally',\n",
       "  'sell',\n",
       "  'source',\n",
       "  'campaign',\n",
       "  'concern',\n",
       "  'sexual',\n",
       "  'escapade',\n",
       "  'bill',\n",
       "  'clinton',\n",
       "  'liken',\n",
       "  'disgrace',\n",
       "  'celebrity',\n",
       "  'bill',\n",
       "  'cosby',\n",
       "  'political',\n",
       "  'operative',\n",
       "  'ron',\n",
       "  'klain',\n",
       "  'send',\n",
       "  'urgent',\n",
       "  'email',\n",
       "  'say',\n",
       "  'hillary',\n",
       "  'anticipate',\n",
       "  'follow',\n",
       "  'question',\n",
       "  'bill',\n",
       "  'clinton',\n",
       "  'different',\n",
       "  'bill',\n",
       "  'cosby',\n",
       "  'conduct',\n",
       "  'relevant',\n",
       "  'campaign',\n",
       "  'say',\n",
       "  'woman',\n",
       "  'believe',\n",
       "  'woman',\n",
       "  'accuse',\n",
       "  'apologize',\n",
       "  'woman',\n",
       "  'wrongly',\n",
       "  'smear',\n",
       "  'husband',\n",
       "  'ally',\n",
       "  'source',\n",
       "  'clinton',\n",
       "  's',\n",
       "  'campaign',\n",
       "  'deliberately',\n",
       "  'leak',\n",
       "  'embarrassing',\n",
       "  'photo',\n",
       "  'swimsuit',\n",
       "  'clothe',\n",
       "  'bernie',\n",
       "  'sander',\n",
       "  'press',\n",
       "  'ironically',\n",
       "  'insinuate',\n",
       "  'proof',\n",
       "  'buy',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'perez',\n",
       "  'hilton',\n",
       "  'write',\n",
       "  'bernie',\n",
       "  'sander',\n",
       "  'lounge',\n",
       "  'elite',\n",
       "  'martha',\n",
       "  's',\n",
       "  'vineyard',\n",
       "  'pool',\n",
       "  'summer',\n",
       "  'help',\n",
       "  'raise',\n",
       "  'money',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'lobbyist',\n",
       "  'source',\n",
       "  'clinton',\n",
       "  'admit',\n",
       "  'touch',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'speech',\n",
       "  'goldman',\n",
       "  'black',\n",
       "  'rock',\n",
       "  'take',\n",
       "  'position',\n",
       "  'policy',\n",
       "  'think',\n",
       "  'grow',\n",
       "  'sense',\n",
       "  'anxiety',\n",
       "  'anger',\n",
       "  'country',\n",
       "  'feeling',\n",
       "  'game',\n",
       "  'rig',\n",
       "  'feeling',\n",
       "  'grow',\n",
       "  'mean',\n",
       "  'rich',\n",
       "  'people',\n",
       "  'course',\n",
       "  'father',\n",
       "  'love',\n",
       "  'complain',\n",
       "  'big',\n",
       "  'business',\n",
       "  'big',\n",
       "  'government',\n",
       "  'solid',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'upbringing',\n",
       "  'good',\n",
       "  'public',\n",
       "  'school',\n",
       "  'accessible',\n",
       "  'health',\n",
       "  'care',\n",
       "  'little',\n",
       "  'know',\n",
       "  'family',\n",
       "  'house',\n",
       "  'know',\n",
       "  'save',\n",
       "  'money',\n",
       "  'didn',\n",
       "  't',\n",
       "  'believe',\n",
       "  'mortgage',\n",
       "  'live',\n",
       "  'obviously',\n",
       "  'm',\n",
       "  'kind',\n",
       "  'far',\n",
       "  'remove',\n",
       "  'life',\n",
       "  've',\n",
       "  'live',\n",
       "  'economic',\n",
       "  'know',\n",
       "  'fortune',\n",
       "  'husband',\n",
       "  'enjoy',\n",
       "  'haven',\n",
       "  't',\n",
       "  'forget',\n",
       "  'source',\n",
       "  'nwo',\n",
       "  'remark',\n",
       "  'pay',\n",
       "  'speech',\n",
       "  'brazilian',\n",
       "  'bank',\n",
       "  'banco',\n",
       "  'itau',\n",
       "  'dream',\n",
       "  'hemispheric',\n",
       "  'common',\n",
       "  'market',\n",
       "  'open',\n",
       "  'trade',\n",
       "  'open',\n",
       "  'border',\n",
       "  'time',\n",
       "  'future',\n",
       "  'energy',\n",
       "  'green',\n",
       "  'sustainable',\n",
       "  'power',\n",
       "  'growth',\n",
       "  'opportunity',\n",
       "  'person',\n",
       "  'hemisphere',\n",
       "  'source',\n",
       "  'leak',\n",
       "  'pay',\n",
       "  'speech',\n",
       "  'time',\n",
       "  'jewish',\n",
       "  'united',\n",
       "  'fund',\n",
       "  'metropolitan',\n",
       "  'chicago',\n",
       "  'clinton',\n",
       "  'say',\n",
       "  'jordan',\n",
       "  'turkey',\n",
       "  't',\n",
       "  'possibly',\n",
       "  'vet',\n",
       "  'refugee',\n",
       "  'don',\n",
       "  't',\n",
       "  'know',\n",
       "  'know',\n",
       "  'jihadist',\n",
       "  'come',\n",
       "  'legitimate',\n",
       "  'refugee',\n",
       "  'clinton',\n",
       "  'way',\n",
       "  'warmly',\n",
       "  'welcome',\n",
       "  'refugee',\n",
       "  'year',\n",
       "  'make',\n",
       "  'obama',\n",
       "  's',\n",
       "  'year',\n",
       "  'look',\n",
       "  'like',\n",
       "  'small',\n",
       "  'potato',\n",
       "  'source',\n",
       "  'clinton',\n",
       "  'blackmail',\n",
       "  'chinese',\n",
       "  'tell',\n",
       "  'base',\n",
       "  'missile',\n",
       "  'region',\n",
       "  'didn',\n",
       "  't',\n",
       "  'exert',\n",
       "  'control',\n",
       "  'north',\n",
       "  'korean',\n",
       "  'aggression',\n",
       "  'china',\n",
       "  'come',\n",
       "  'control',\n",
       "  'go',\n",
       "  'defend',\n",
       "  'purportedly',\n",
       "  'tell',\n",
       "  'audience',\n",
       "  'goldman',\n",
       "  'sachs',\n",
       "  'conference',\n",
       "  'june',\n",
       "  'source',\n",
       "  'clinton',\n",
       "  'long',\n",
       "  'secretary',\n",
       "  'state',\n",
       "  'ready',\n",
       "  'announce',\n",
       "  'run',\n",
       "  'president',\n",
       "  'invite',\n",
       "  'attend',\n",
       "  'summit',\n",
       "  'morrocco',\n",
       "  'implication',\n",
       "  'leak',\n",
       "  'email',\n",
       "  'million',\n",
       "  'donation',\n",
       "  'king',\n",
       "  'morocco',\n",
       "  'dependent',\n",
       "  'clinton',\n",
       "  'attend',\n",
       "  'summit',\n",
       "  'human',\n",
       "  'abedin',\n",
       "  'usually',\n",
       "  'loyal',\n",
       "  'boss',\n",
       "  'concern',\n",
       "  'hrc',\n",
       "  'meeting',\n",
       "  'non',\n",
       "  'starter',\n",
       "  'create',\n",
       "  'mess',\n",
       "  'know',\n",
       "  'presence',\n",
       "  'condition',\n",
       "  'moroccan',\n",
       "  'proceed',\n",
       "  'go',\n",
       "  'abedin',\n",
       "  'write',\n",
       "  'robbie',\n",
       "  'mook',\n",
       "  'november',\n",
       "  'email',\n",
       "  'incidentally',\n",
       "  'clinton',\n",
       "  'didn',\n",
       "  't',\n",
       "  'attend',\n",
       "  'bill',\n",
       "  'chelsea',\n",
       "  'go',\n",
       "  'instead',\n",
       "  'million',\n",
       "  'donation',\n",
       "  'forthcoming',\n",
       "  'source',\n",
       "  'podesta',\n",
       "  'attack',\n",
       "  'clinton',\n",
       "  's',\n",
       "  'primary',\n",
       "  'election',\n",
       "  'rival',\n",
       "  'bernie',\n",
       "  'sander',\n",
       "  'criticize',\n",
       "  'paris',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'agreement',\n",
       "  'believe',\n",
       "  'doofus',\n",
       "  'bernie',\n",
       "  'attack',\n",
       "  'say',\n",
       "  'podesta',\n",
       "  'source',\n",
       "  'clinton',\n",
       "  'tell',\n",
       "  'goldman',\n",
       "  'sach',\n",
       "  'conference',\n",
       "  'like',\n",
       "  'intervene',\n",
       "  'secretly',\n",
       "  'syria',\n",
       "  'view',\n",
       "  'intervene',\n",
       "  'covertly',\n",
       "  'possible',\n",
       "  'american',\n",
       "  'intervene',\n",
       "  'tell',\n",
       "  'employee',\n",
       "  'bank',\n",
       "  'south',\n",
       "  'carolina',\n",
       "  'pay',\n",
       "  'speech',\n",
       "  'good',\n",
       "  'know',\n",
       "  'everybody',\n",
       "  't',\n",
       "  'help',\n",
       "  'tell',\n",
       "  'friendly',\n",
       "  'reporter',\n",
       "  'somebody',\n",
       "  'look',\n",
       "  'want',\n",
       "  'credit',\n",
       "  'source',\n",
       "  'definite',\n",
       "  'link',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'msm',\n",
       "  'allow',\n",
       "  'campaign',\n",
       "  'collude',\n",
       "  'directly',\n",
       "  'medium',\n",
       "  'spokesperson',\n",
       "  'read',\n",
       "  'like',\n",
       "  's',\n",
       "  'american',\n",
       "  'medium',\n",
       "  'dan',\n",
       "  'merica',\n",
       "  'cnn',\n",
       "  'haim',\n",
       "  'saban',\n",
       "  'univision',\n",
       "  'john',\n",
       "  'harwood',\n",
       "  'cnbc',\n",
       "  'ny',\n",
       "  'time',\n",
       "  'rebecca',\n",
       "  'quick',\n",
       "  'cnbc',\n",
       "  'maggie',\n",
       "  'haberman',\n",
       "  'ny',\n",
       "  'time',\n",
       "  'politico',\n",
       "  'john',\n",
       "  'harris',\n",
       "  'politico',\n",
       "  'donna',\n",
       "  'brazile',\n",
       "  'cnn',\n",
       "  'roland',\n",
       "  'martin',\n",
       "  'tv',\n",
       "  'marjorie',\n",
       "  'pritchard',\n",
       "  'boston',\n",
       "  'globe',\n",
       "  'louise',\n",
       "  'mensch',\n",
       "  'heat',\n",
       "  'street',\n",
       "  'source',\n",
       "  'know',\n",
       "  'dnc',\n",
       "  'deliberately',\n",
       "  'screw',\n",
       "  'bernie',\n",
       "  'sander',\n",
       "  'nomination',\n",
       "  'bonus',\n",
       "  'wikileak',\n",
       "  'release',\n",
       "  'dnc',\n",
       "  's',\n",
       "  'voicemail',\n",
       "  'topic',\n",
       "  'email',\n",
       "  'prove',\n",
       "  'actually',\n",
       "  'pull',\n",
       "  'hrc',\n",
       "  's',\n",
       "  'puppet',\n",
       "  'string',\n",
       "  'puppeteer',\n",
       "  'george',\n",
       "  'soro',\n",
       "  'shadow',\n",
       "  'government',\n",
       "  'conspiracy',\n",
       "  'theory',\n",
       "  'exist',\n",
       "  'hillary',\n",
       "  's',\n",
       "  'job',\n",
       "  'george',\n",
       "  'soro',\n",
       "  'happy',\n",
       "  'source',\n",
       "  'excerpt',\n",
       "  'speech',\n",
       "  'wall',\n",
       "  'street',\n",
       "  'read',\n",
       "  'like',\n",
       "  'guide',\n",
       "  'faced',\n",
       "  'treachery',\n",
       "  'clearly',\n",
       "  'point',\n",
       "  'need',\n",
       "  'lie',\n",
       "  'everybody',\n",
       "  's',\n",
       "  'watch',\n",
       "  'know',\n",
       "  'room',\n",
       "  'discussion',\n",
       "  'deal',\n",
       "  'know',\n",
       "  'people',\n",
       "  'little',\n",
       "  'nervous',\n",
       "  'need',\n",
       "  'public',\n",
       "  'private',\n",
       "  'position',\n",
       "  'source',\n",
       "  'wikileak',\n",
       "  'email',\n",
       "  'work',\n",
       "  'cnn',\n",
       "  'employee',\n",
       "  'clinton',\n",
       "  'campaign',\n",
       "  'donna',\n",
       "  'brazile',\n",
       "  'give',\n",
       "  'hillary',\n",
       "  'question',\n",
       "  'advance',\n",
       "  'impromptu',\n",
       "  'cnn',\n",
       "  'town',\n",
       "  'hall',\n",
       "  'question',\n",
       "  'source',\n",
       "  'campaign',\n",
       "  'get',\n",
       "  'approve',\n",
       "  'article',\n",
       "  'influential',\n",
       "  'publication',\n",
       "  'like',\n",
       "  'ny',\n",
       "  'time',\n",
       "  'huffpo',\n",
       "  'cnn',\n",
       "  'nbc',\n",
       "  'cbs',\n",
       "  'nyt',\n",
       "  'msnbc',\n",
       "  'politico',\n",
       "  'show',\n",
       "  'massive',\n",
       "  'collusion',\n",
       "  'mainstream',\n",
       "  'medium',\n",
       "  'hound',\n",
       "  'trump',\n",
       "  'relentlessly',\n",
       "  'effort',\n",
       "  'distract',\n",
       "  'hrc',\n",
       "  's',\n",
       "  'abysmal',\n",
       "  'candidacy',\n",
       "  'source',\n",
       "  'treasure',\n",
       "  'trove',\n",
       "  'wikileak',\n",
       "  'email',\n",
       "  'gain',\n",
       "  'accurate',\n",
       "  'picture',\n",
       "  'hillary',\n",
       "  'feel',\n",
       "  'spoiler',\n",
       "  'basket',\n",
       "  'deplorable',\n",
       "  'basement',\n",
       "  'dweller',\n",
       "  'right',\n",
       "  'wing',\n",
       "  'conspirator',\n",
       "  'source',\n",
       "  'president',\n",
       "  'obama',\n",
       "  'know',\n",
       "  'time',\n",
       "  'email',\n",
       "  'come',\n",
       "  'secure',\n",
       "  'state',\n",
       "  'department',\n",
       "  'server',\n",
       "  'cheryl',\n",
       "  'mill',\n",
       "  'write',\n",
       "  'john',\n",
       "  'podesta',\n",
       "  'w',\n",
       "  'e',\n",
       "  'need',\n",
       "  'clean',\n",
       "  'email',\n",
       "  'state',\n",
       "  'gov',\n",
       "  'obama',\n",
       "  's',\n",
       "  'email',\n",
       "  'whitelist',\n",
       "  'address',\n",
       "  'add',\n",
       "  'nonsecure',\n",
       "  'email',\n",
       "  'whitelist',\n",
       "  'source',\n",
       "  'finally',\n",
       "  's',\n",
       "  'real',\n",
       "  'reason',\n",
       "  'treacherous',\n",
       "  'shrew',\n",
       "  'involve',\n",
       "  'politic',\n",
       "  'let',\n",
       "  'tell',\n",
       "  'isn',\n",
       "  't',\n",
       "  'yearn',\n",
       "  'thing',\n",
       "  'good',\n",
       "  'emphasis',\n",
       "  'goldman',\n",
       "  'sachs',\n",
       "  'builder',\n",
       "  'innovator',\n",
       "  'summit',\n",
       "  'clinton',\n",
       "  'respond',\n",
       "  'question',\n",
       "  'chief',\n",
       "  'executive',\n",
       "  'lloyd',\n",
       "  'blankfein',\n",
       "  'quip',\n",
       "  'washington',\n",
       "  'small',\n",
       "  'fortune',\n",
       "  'clinton',\n",
       "  'agree',\n",
       "  'comment',\n",
       "  'complain',\n",
       "  'ethic',\n",
       "  'rule',\n",
       "  'require',\n",
       "  'official',\n",
       "  'divest',\n",
       "  'certain',\n",
       "  'asset',\n",
       "  'enter',\n",
       "  'government',\n",
       "  'bias',\n",
       "  'people',\n",
       "  'lead',\n",
       "  'successful',\n",
       "  'complicated',\n",
       "  'life',\n",
       "  'clinton',\n",
       "  'say',\n",
       "  'source',\n",
       "  'ignore',\n",
       "  'mission',\n",
       "  'presidential',\n",
       "  'election',\n",
       "  'november',\n",
       "  'th',\n",
       "  'hope',\n",
       "  'join',\n",
       "  'go',\n",
       "  'work',\n",
       "  'day',\n",
       "  'night',\n",
       "  'provide',\n",
       "  'coverage',\n",
       "  'mainstream',\n",
       "  'medium',\n",
       "  'isn',\n",
       "  't',\n",
       "  'combine',\n",
       "  'voice',\n",
       "  'people',\n",
       "  'listen',\n",
       "  'scandal',\n",
       "  'rigging',\n",
       "  'corruption',\n",
       "  'election',\n",
       "  'system',\n",
       "  'general',\n",
       "  'join',\n",
       "  'voice',\n",
       "  'like',\n",
       "  'sharing',\n",
       "  'spread',\n",
       "  'word',\n",
       "  'ignore',\n",
       "  'army',\n",
       "  'courtesy',\n",
       "  'daisy',\n",
       "  'luther',\n",
       "  'not',\n",
       "  'forget',\n",
       "  'follow',\n",
       "  'd',\n",
       "  'c',\n",
       "  'clothesline',\n",
       "  'facebook',\n",
       "  'twitter',\n",
       "  'help',\n",
       "  'spread',\n",
       "  'word',\n",
       "  'share',\n",
       "  'article',\n",
       "  'favorite',\n",
       "  'social',\n",
       "  'network',\n",
       "  'share'],\n",
       " ...]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "sent = [row.split() for row in df_clean['clean']]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 16:27:58: collecting all words and their counts\n",
      "INFO - 16:27:58: PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO - 16:28:02: collected 1040505 word types from a corpus of 1659052 words (unigram + bigrams) and 4597 sentences\n",
      "INFO - 16:28:02: using 1040505 counts as vocab in Phrases<0 vocab, min_count=30, threshold=10.0, max_vocab_size=40000000>\n"
     ]
    }
   ],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = phrases[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "69694"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['s',\n",
       " 'say',\n",
       " 'people',\n",
       " 'trump',\n",
       " 'time',\n",
       " 'know',\n",
       " 'state',\n",
       " 'like',\n",
       " 'clinton',\n",
       " 'go']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO - 16:28:13: collecting all words and their counts\n",
      "INFO - 16:28:13: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 16:28:21: collected 69694 word types from a corpus of 1570970 raw words and 4597 sentences\n",
      "INFO - 16:28:21: Loading a fresh vocabulary\n",
      "INFO - 16:28:21: effective_min_count=20 retains 8916 unique words (12% of original 69694, drops 60778)\n",
      "INFO - 16:28:21: effective_min_count=20 leaves 1382608 word corpus (88% of original 1570970, drops 188362)\n",
      "INFO - 16:28:21: deleting the raw counts dictionary of 69694 items\n",
      "INFO - 16:28:21: sample=6e-05 downsamples 1369 most-common words\n",
      "INFO - 16:28:21: downsampling leaves estimated 875651 word corpus (63.3% of prior 1382608)\n",
      "INFO - 16:28:21: estimated required memory for 8916 words and 300 dimensions: 25856400 bytes\n",
      "INFO - 16:28:21: resetting layer weights\n",
      "Time to build vocab: 0.2 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "hreads\n",
      "INFO - 16:28:59: EPOCH - 2 : training on 1570970 raw words (875754 effective words) took 17.4s, 50345 effective words/s\n",
      "INFO - 16:29:00: EPOCH 3 - PROGRESS: at 6.03% examples, 36822 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:01: EPOCH 3 - PROGRESS: at 11.94% examples, 39951 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:02: EPOCH 3 - PROGRESS: at 16.25% examples, 38844 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:03: EPOCH 3 - PROGRESS: at 23.02% examples, 37180 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:04: EPOCH 3 - PROGRESS: at 28.80% examples, 37056 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:05: EPOCH 3 - PROGRESS: at 33.00% examples, 37488 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:06: EPOCH 3 - PROGRESS: at 36.18% examples, 38081 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:07: EPOCH 3 - PROGRESS: at 44.70% examples, 38524 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:08: EPOCH 3 - PROGRESS: at 48.29% examples, 38089 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:09: EPOCH 3 - PROGRESS: at 53.06% examples, 37416 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:10: EPOCH 3 - PROGRESS: at 56.38% examples, 38001 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:11: EPOCH 3 - PROGRESS: at 62.63% examples, 39316 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:12: EPOCH 3 - PROGRESS: at 72.13% examples, 43202 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:14: EPOCH 3 - PROGRESS: at 77.12% examples, 46917 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:15: EPOCH 3 - PROGRESS: at 85.58% examples, 47707 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:16: EPOCH 3 - PROGRESS: at 91.76% examples, 49277 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:16: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:29:16: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:29:16: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:16: EPOCH - 3 : training on 1570970 raw words (875881 effective words) took 17.2s, 50900 effective words/s\n",
      "INFO - 16:29:17: EPOCH 4 - PROGRESS: at 10.51% examples, 58522 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:18: EPOCH 4 - PROGRESS: at 22.43% examples, 71417 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:19: EPOCH 4 - PROGRESS: at 32.65% examples, 73365 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:20: EPOCH 4 - PROGRESS: at 35.09% examples, 64759 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:21: EPOCH 4 - PROGRESS: at 44.33% examples, 61463 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:22: EPOCH 4 - PROGRESS: at 53.06% examples, 63240 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:23: EPOCH 4 - PROGRESS: at 59.06% examples, 65042 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:24: EPOCH 4 - PROGRESS: at 69.94% examples, 65691 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:25: EPOCH 4 - PROGRESS: at 72.29% examples, 64117 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:26: EPOCH 4 - PROGRESS: at 72.98% examples, 61173 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:28: EPOCH 4 - PROGRESS: at 77.96% examples, 60853 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:29: EPOCH 4 - PROGRESS: at 86.16% examples, 60695 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:30: EPOCH 4 - PROGRESS: at 93.04% examples, 61071 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:30: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:29:30: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:29:30: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:30: EPOCH - 4 : training on 1570970 raw words (876030 effective words) took 14.3s, 61446 effective words/s\n",
      "INFO - 16:29:32: EPOCH 5 - PROGRESS: at 11.62% examples, 69493 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:33: EPOCH 5 - PROGRESS: at 17.71% examples, 59394 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:34: EPOCH 5 - PROGRESS: at 28.80% examples, 58278 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:35: EPOCH 5 - PROGRESS: at 34.67% examples, 60840 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:36: EPOCH 5 - PROGRESS: at 43.31% examples, 58228 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:29:37: EPOCH 5 - PROGRESS: at 52.19% examples, 60240 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:38: EPOCH 5 - PROGRESS: at 57.76% examples, 61007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:39: EPOCH 5 - PROGRESS: at 68.54% examples, 63307 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:40: EPOCH 5 - PROGRESS: at 72.35% examples, 64519 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:41: EPOCH 5 - PROGRESS: at 77.55% examples, 65527 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:42: EPOCH 5 - PROGRESS: at 85.58% examples, 64471 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:43: EPOCH 5 - PROGRESS: at 89.19% examples, 63195 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:44: EPOCH 5 - PROGRESS: at 97.65% examples, 62935 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:44: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:29:44: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:29:44: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:44: EPOCH - 5 : training on 1570970 raw words (875670 effective words) took 13.8s, 63296 effective words/s\n",
      "INFO - 16:29:45: EPOCH 6 - PROGRESS: at 11.94% examples, 81796 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:46: EPOCH 6 - PROGRESS: at 25.36% examples, 83051 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:47: EPOCH 6 - PROGRESS: at 33.30% examples, 79022 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:48: EPOCH 6 - PROGRESS: at 43.31% examples, 77118 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:49: EPOCH 6 - PROGRESS: at 54.12% examples, 78642 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:50: EPOCH 6 - PROGRESS: at 62.63% examples, 79532 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:51: EPOCH 6 - PROGRESS: at 71.92% examples, 79321 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:53: EPOCH 6 - PROGRESS: at 75.22% examples, 78635 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:54: EPOCH 6 - PROGRESS: at 83.01% examples, 77937 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:55: EPOCH 6 - PROGRESS: at 88.86% examples, 75826 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:56: EPOCH 6 - PROGRESS: at 93.74% examples, 72470 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:56: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:29:56: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:29:56: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:29:56: EPOCH - 6 : training on 1570970 raw words (876140 effective words) took 12.2s, 71745 effective words/s\n",
      "INFO - 16:29:58: EPOCH 7 - PROGRESS: at 9.88% examples, 50416 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:29:59: EPOCH 7 - PROGRESS: at 15.86% examples, 55257 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:00: EPOCH 7 - PROGRESS: at 27.56% examples, 57975 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:01: EPOCH 7 - PROGRESS: at 34.67% examples, 64106 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:02: EPOCH 7 - PROGRESS: at 47.42% examples, 68752 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:03: EPOCH 7 - PROGRESS: at 56.38% examples, 70898 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:04: EPOCH 7 - PROGRESS: at 64.61% examples, 70175 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:05: EPOCH 7 - PROGRESS: at 70.44% examples, 67958 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:06: EPOCH 7 - PROGRESS: at 72.61% examples, 68175 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:07: EPOCH 7 - PROGRESS: at 78.99% examples, 69641 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:08: EPOCH 7 - PROGRESS: at 89.19% examples, 70442 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:09: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:30:09: EPOCH 7 - PROGRESS: at 99.20% examples, 70721 words/s, in_qsize 1, out_qsize 1\n",
      "INFO - 16:30:09: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:30:09: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:09: EPOCH - 7 : training on 1570970 raw words (875997 effective words) took 12.4s, 70865 effective words/s\n",
      "INFO - 16:30:10: EPOCH 8 - PROGRESS: at 11.62% examples, 74746 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:11: EPOCH 8 - PROGRESS: at 23.02% examples, 74300 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:12: EPOCH 8 - PROGRESS: at 33.00% examples, 74386 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:13: EPOCH 8 - PROGRESS: at 42.01% examples, 73600 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:14: EPOCH 8 - PROGRESS: at 50.58% examples, 71279 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:15: EPOCH 8 - PROGRESS: at 56.71% examples, 70145 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:16: EPOCH 8 - PROGRESS: at 62.63% examples, 67163 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:17: EPOCH 8 - PROGRESS: at 70.15% examples, 66085 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:18: EPOCH 8 - PROGRESS: at 72.48% examples, 65812 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:19: EPOCH 8 - PROGRESS: at 77.12% examples, 65995 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:20: EPOCH 8 - PROGRESS: at 86.16% examples, 66046 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:21: EPOCH 8 - PROGRESS: at 89.80% examples, 64177 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:22: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:30:22: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:30:22: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:22: EPOCH - 8 : training on 1570970 raw words (875691 effective words) took 13.4s, 65197 effective words/s\n",
      "INFO - 16:30:23: EPOCH 9 - PROGRESS: at 10.81% examples, 62045 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:24: EPOCH 9 - PROGRESS: at 23.02% examples, 73482 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:25: EPOCH 9 - PROGRESS: at 30.26% examples, 66779 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:26: EPOCH 9 - PROGRESS: at 36.18% examples, 66236 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:28: EPOCH 9 - PROGRESS: at 45.51% examples, 63551 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:29: EPOCH 9 - PROGRESS: at 53.64% examples, 63113 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:30: EPOCH 9 - PROGRESS: at 59.80% examples, 64717 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:31: EPOCH 9 - PROGRESS: at 69.94% examples, 65049 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:32: EPOCH 9 - PROGRESS: at 72.35% examples, 65007 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:33: EPOCH 9 - PROGRESS: at 76.46% examples, 64412 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:34: EPOCH 9 - PROGRESS: at 83.01% examples, 63429 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:35: EPOCH 9 - PROGRESS: at 88.41% examples, 62452 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:36: EPOCH 9 - PROGRESS: at 91.76% examples, 60539 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:37: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:30:37: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:30:37: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:37: EPOCH - 9 : training on 1570970 raw words (875658 effective words) took 14.4s, 60805 effective words/s\n",
      "INFO - 16:30:38: EPOCH 10 - PROGRESS: at 9.88% examples, 49143 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:39: EPOCH 10 - PROGRESS: at 15.44% examples, 51959 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:40: EPOCH 10 - PROGRESS: at 26.95% examples, 55830 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:41: EPOCH 10 - PROGRESS: at 33.96% examples, 60403 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:42: EPOCH 10 - PROGRESS: at 45.07% examples, 63375 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:43: EPOCH 10 - PROGRESS: at 55.64% examples, 66843 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:44: EPOCH 10 - PROGRESS: at 62.63% examples, 67306 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:45: EPOCH 10 - PROGRESS: at 71.94% examples, 68897 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:46: EPOCH 10 - PROGRESS: at 75.81% examples, 70459 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:47: EPOCH 10 - PROGRESS: at 84.92% examples, 70674 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:48: EPOCH 10 - PROGRESS: at 90.54% examples, 71415 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:49: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:30:49: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:30:49: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:30:49: EPOCH - 10 : training on 1570970 raw words (875790 effective words) took 12.3s, 71131 effective words/s\n",
      "INFO - 16:30:50: EPOCH 11 - PROGRESS: at 11.94% examples, 79093 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:51: EPOCH 11 - PROGRESS: at 26.21% examples, 81966 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:52: EPOCH 11 - PROGRESS: at 33.52% examples, 78690 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:53: EPOCH 11 - PROGRESS: at 43.31% examples, 74693 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:54: EPOCH 11 - PROGRESS: at 50.58% examples, 71459 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:55: EPOCH 11 - PROGRESS: at 58.32% examples, 72981 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:56: EPOCH 11 - PROGRESS: at 69.50% examples, 73673 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:30:57: EPOCH 11 - PROGRESS: at 72.46% examples, 73675 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:58: EPOCH 11 - PROGRESS: at 77.96% examples, 74691 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:30:59: EPOCH 11 - PROGRESS: at 88.04% examples, 75314 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:00: EPOCH 11 - PROGRESS: at 95.11% examples, 73976 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:01: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:31:01: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:31:01: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:01: EPOCH - 11 : training on 1570970 raw words (876526 effective words) took 11.8s, 74485 effective words/s\n",
      "INFO - 16:31:02: EPOCH 12 - PROGRESS: at 11.94% examples, 80274 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:03: EPOCH 12 - PROGRESS: at 24.71% examples, 78634 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:04: EPOCH 12 - PROGRESS: at 33.52% examples, 79363 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:05: EPOCH 12 - PROGRESS: at 45.07% examples, 79884 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:06: EPOCH 12 - PROGRESS: at 54.69% examples, 78910 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:07: EPOCH 12 - PROGRESS: at 63.48% examples, 79850 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:08: EPOCH 12 - PROGRESS: at 71.92% examples, 78700 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:09: EPOCH 12 - PROGRESS: at 75.64% examples, 79167 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:10: EPOCH 12 - PROGRESS: at 85.29% examples, 79378 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:11: EPOCH 12 - PROGRESS: at 90.12% examples, 77641 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:12: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:31:12: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:31:12: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:12: EPOCH - 12 : training on 1570970 raw words (875369 effective words) took 11.2s, 78237 effective words/s\n",
      "INFO - 16:31:13: EPOCH 13 - PROGRESS: at 10.81% examples, 64596 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:14: EPOCH 13 - PROGRESS: at 17.71% examples, 63477 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:15: EPOCH 13 - PROGRESS: at 28.26% examples, 59879 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:16: EPOCH 13 - PROGRESS: at 33.30% examples, 58244 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:17: EPOCH 13 - PROGRESS: at 44.33% examples, 60339 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:18: EPOCH 13 - PROGRESS: at 52.19% examples, 61411 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:19: EPOCH 13 - PROGRESS: at 56.38% examples, 59132 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:20: EPOCH 13 - PROGRESS: at 63.48% examples, 58676 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:22: EPOCH 13 - PROGRESS: at 71.02% examples, 58413 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:23: EPOCH 13 - PROGRESS: at 72.33% examples, 56684 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:24: EPOCH 13 - PROGRESS: at 75.64% examples, 55978 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:25: EPOCH 13 - PROGRESS: at 77.12% examples, 53920 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:26: EPOCH 13 - PROGRESS: at 85.29% examples, 53781 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:27: EPOCH 13 - PROGRESS: at 89.56% examples, 53651 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:28: EPOCH 13 - PROGRESS: at 96.21% examples, 53129 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:28: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:31:28: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:31:28: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:28: EPOCH - 13 : training on 1570970 raw words (876184 effective words) took 16.3s, 53800 effective words/s\n",
      "INFO - 16:31:29: EPOCH 14 - PROGRESS: at 11.94% examples, 78956 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:30: EPOCH 14 - PROGRESS: at 23.78% examples, 77214 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:31: EPOCH 14 - PROGRESS: at 33.52% examples, 78977 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:32: EPOCH 14 - PROGRESS: at 45.51% examples, 81008 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:33: EPOCH 14 - PROGRESS: at 54.12% examples, 78200 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:34: EPOCH 14 - PROGRESS: at 62.63% examples, 79295 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:35: EPOCH 14 - PROGRESS: at 71.74% examples, 78726 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:37: EPOCH 14 - PROGRESS: at 75.22% examples, 78673 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:38: EPOCH 14 - PROGRESS: at 84.92% examples, 79037 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:39: EPOCH 14 - PROGRESS: at 89.56% examples, 77428 words/s, in_qsize 1, out_qsize 0\n",
      "INFO - 16:31:39: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:31:39: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:31:40: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:40: EPOCH - 14 : training on 1570970 raw words (875383 effective words) took 11.2s, 77952 effective words/s\n",
      "INFO - 16:31:41: EPOCH 15 - PROGRESS: at 11.94% examples, 80431 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:42: EPOCH 15 - PROGRESS: at 23.78% examples, 78151 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:43: EPOCH 15 - PROGRESS: at 33.00% examples, 76461 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:44: EPOCH 15 - PROGRESS: at 44.33% examples, 76024 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:45: EPOCH 15 - PROGRESS: at 54.12% examples, 76611 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:46: EPOCH 15 - PROGRESS: at 59.80% examples, 75141 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:47: EPOCH 15 - PROGRESS: at 69.94% examples, 74190 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:48: EPOCH 15 - PROGRESS: at 72.48% examples, 73927 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:49: EPOCH 15 - PROGRESS: at 77.55% examples, 73393 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:50: EPOCH 15 - PROGRESS: at 86.51% examples, 73158 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:51: EPOCH 15 - PROGRESS: at 95.11% examples, 73639 words/s, in_qsize 0, out_qsize 0\n",
      "INFO - 16:31:51: worker thread finished; awaiting finish of 2 more threads\n",
      "INFO - 16:31:51: worker thread finished; awaiting finish of 1 more threads\n",
      "INFO - 16:31:51: worker thread finished; awaiting finish of 0 more threads\n",
      "INFO - 16:31:51: EPOCH - 15 : training on 1570970 raw words (875733 effective words) took 11.9s, 73732 effective words/s\n",
      "INFO - 16:31:51: training on a 23564550 raw words (13137930 effective words) took 207.1s, 63439 effective words/s\n",
      "Time to train the model: 3.45 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=15, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('hoax', 0.6655097603797913),\n",
       " ('fake_news', 0.6599137783050537),\n",
       " ('circulate', 0.6320031881332397),\n",
       " ('biased', 0.6317537426948547),\n",
       " ('sicken', 0.6033468246459961),\n",
       " ('scam', 0.6008330583572388),\n",
       " ('misinformation', 0.6002168655395508),\n",
       " ('proof', 0.6000297665596008),\n",
       " ('false', 0.5967258214950562),\n",
       " ('troll', 0.5930442810058594)]"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"fake\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('liberty', 0.7934807538986206),\n",
       " ('tyranny', 0.7433617115020752),\n",
       " ('deprive', 0.7103211879730225),\n",
       " ('bear_arm', 0.6700611710548401),\n",
       " ('freedom_speech', 0.6616452932357788),\n",
       " ('dignity', 0.6511629819869995),\n",
       " ('equality', 0.6453904509544373),\n",
       " ('preserve', 0.6435351371765137),\n",
       " ('uphold', 0.639883816242218),\n",
       " ('proclaim', 0.6337151527404785)]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"freedom\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('false_flag', 0.7994900345802307),\n",
       " ('bloody', 0.7829836010932922),\n",
       " ('terrorist_attack', 0.7823898792266846),\n",
       " ('isis_fighter', 0.7490600943565369),\n",
       " ('warplane', 0.7447312474250793),\n",
       " ('relentless', 0.7430627346038818),\n",
       " ('taliban', 0.7392674684524536),\n",
       " ('houthi', 0.7388142943382263),\n",
       " ('barrage', 0.7347888946533203),\n",
       " ('behead', 0.7337174415588379),\n",
       " ('aftermath', 0.7299622893333435),\n",
       " ('helmet', 0.7236400842666626),\n",
       " ('violently', 0.7234197854995728),\n",
       " ('airstrike', 0.7209673523902893),\n",
       " ('meantime', 0.7197535037994385),\n",
       " ('lead_coalition', 0.7163382768630981),\n",
       " ('houthis', 0.7148869037628174),\n",
       " ('ypg', 0.7146459817886353),\n",
       " ('war_crime', 0.7138177752494812),\n",
       " ('bombing', 0.7130325436592102)]"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"attack\"],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('atomic', 0.7754235863685608),\n",
       " ('warplane', 0.7604361772537231),\n",
       " ('yeman', 0.7586443424224854),\n",
       " ('strike', 0.7105411887168884),\n",
       " ('shelter', 0.6895444393157959),\n",
       " ('air_strike', 0.681679904460907),\n",
       " ('sniper', 0.6816248893737793),\n",
       " ('isis_fighter', 0.6776495575904846),\n",
       " ('airstrike', 0.66100013256073),\n",
       " ('artillery', 0.6574423909187317),\n",
       " ('houthis', 0.6569006443023682),\n",
       " ('yemeni', 0.6505181789398193),\n",
       " ('drop', 0.6471607685089111),\n",
       " ('bullet', 0.6449828147888184),\n",
       " ('cluster', 0.6388813853263855),\n",
       " ('bomber', 0.6362646818161011),\n",
       " ('taliban', 0.6357870101928711),\n",
       " ('syria', 0.6310577988624573),\n",
       " ('rocket', 0.6288050413131714),\n",
       " ('north_korea', 0.6275111436843872)]"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"bomb\"],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('ronald_reagan', 0.797315239906311),\n",
       " ('predecessor', 0.7641506195068359),\n",
       " ('presidency', 0.7636823654174805),\n",
       " ('joe_biden', 0.758105993270874),\n",
       " ('president_elect', 0.7563950419425964),\n",
       " ('white_house', 0.7563837766647339),\n",
       " ('impeach', 0.7525460720062256),\n",
       " ('commander_chief', 0.7493503093719482),\n",
       " ('elect_president', 0.7470866441726685),\n",
       " ('transition_team', 0.7453500032424927)]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"president\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('ronald_reagan', 0.797315239906311),\n",
       " ('predecessor', 0.7641506195068359),\n",
       " ('presidency', 0.7636823654174805),\n",
       " ('joe_biden', 0.758105993270874),\n",
       " ('president_elect', 0.7563950419425964),\n",
       " ('white_house', 0.7563837766647339),\n",
       " ('impeach', 0.7525460720062256),\n",
       " ('commander_chief', 0.7493503093719482),\n",
       " ('elect_president', 0.7470866441726685),\n",
       " ('transition_team', 0.7453500032424927)]"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"president\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}